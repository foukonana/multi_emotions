{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "d61ca32f69b196e39726d48db934cc6efa84f77269006caf23944fce99b0cb54"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Multi-label classification  \n",
    "\n",
    "Often, we may encouter data that can be classified into more than one categories (for example movie genre, items in an image).  \n",
    "However, typical classification tasks involve predicting a single label, as they treat classes as being mutually exclusive.   \n",
    "\n",
    "Multi-Label Classification is the supervised learning problem where an instance may be associated with multiple labels. This is opposed to the traditional task of single-label classification (i.e., multi-class, or binary) where each instance is only associated with a single class label. \n",
    "\n",
    "  \n",
    "\n",
    "### Techniques   \n",
    "\n",
    "There are two main categorizations of methods that can be used to solve for the multi-label classification problem  \n",
    "* problem transformation methods and \n",
    "* algorithm adaptation methods \n",
    "\n",
    "In the first case the learning task is transformed into more or single-label classification tasks. \n",
    "In the second, the algorithms are adapted so that they can handle multi-label data.   \n",
    "\n",
    "\n",
    "<br />\n",
    "\n",
    "The dataset used here is the GoEmotions.  \n",
    "This is a dataset released from Google and it containes the emotions detected in those texts.  \n",
    "It is the largest manually annotated dataset of 58K English Reddit comments, labeled for 27 emotion categories or neutral.  \n",
    "Find the paper on [arXiv.org](https://arxiv.org/abs/2005.00547)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "import re \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pathlib.Path.cwd() / 'Datasets/train.tsv'\n",
    "df = pd.read_csv(dataset, sep='\\t', header=None, names=['comment', 'label', 'id'])\n",
    "df['label'] = df['label'].str.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_list = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment',                     'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism',                 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
    "\n",
    "enkman_mapping = {\n",
    "        \"anger\": [\"anger\", \"annoyance\", \"disapproval\"],\n",
    "        \"disgust\": [\"disgust\"],\n",
    "        \"fear\": [\"fear\", \"nervousness\"],\n",
    "        \"joy\": [\"joy\", \"amusement\", \"approval\", \"excitement\", \"gratitude\",  \"love\", \"optimism\", \"relief\", \"pride\", \"admiration\", \"desire\",                       \"caring\"],\n",
    "        \"sadness\": [\"sadness\", \"disappointment\", \"embarrassment\", \"grief\",  \"remorse\"],\n",
    "        \"surprise\": [\"surprise\", \"realization\", \"confusion\", \"curiosity\"],\n",
    "        \"neutral\": [\"neutral\"],\n",
    "        }\n",
    "enkman_mapping_rev = {v:key for key, value in enkman_mapping.items() for v in value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function from Google Research analysis \n",
    "def idx2class(idx_list):\n",
    "    arr = []\n",
    "    for i in idx_list:\n",
    "        arr.append(emotion_list[int(i)])\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add emotion label to the label ids\n",
    "df['emotions'] = df['label'].apply(idx2class)\n",
    "\n",
    "# use enkman mapping to reduce the emotions to a list of ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'neutral']\n",
    "df['mapped_emotions'] = df['emotions'].apply(lambda x: [enkman_mapping_rev[i] for i in x])\n",
    "\n",
    "# fix issues where ['joy',' joy'] might appear\n",
    "df.loc[df['mapped_emotions'].apply(len)>1, 'mapped_emotions'] = df.loc[df['mapped_emotions'].apply(len)>1, 'mapped_emotions'].apply(lambda x: [emotion for emotion in set(x)])"
   ]
  },
  {
   "source": [
    "(simple) text pre-processing and TF_IDF representation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "_NOTICE_   \n",
    "r/ represents a reddit category   \n",
    "Example: 'r/hockey has no love for us! Just stay here with all us cool people!'\n",
    "\n",
    "\\[NAME] is replaced from a word that may be representing a brand or a person  \n",
    "Example: 'How have \\[NAME] and \\[NAME] looked tonight? I was watching the Huskies game during the first period.'"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stopword_list = stopwords.words('english')\n",
    "\n",
    "\n",
    "def process_reddit_comment(strng):\n",
    "    # remove [NAME] placeholder\n",
    "    processed_strng = re.sub('\\[name]', '', strng)\n",
    "    # remove reddit symbol \n",
    "    processed_strng = re.sub('/r', '', processed_strng)\n",
    "    return processed_strng\n",
    "\n",
    "\n",
    "def punct_remover(strng):\n",
    "    # punctuation marks to be completely removed\n",
    "    clean_strng = re.sub(r'[?|!|\\'|\"|#]', r'', strng)\n",
    "    # punctuation marks to be replaced with space\n",
    "    clean_strng = re.sub(r'[.|,|)|(|\\|/]', r' ', clean_strng)\n",
    "    # replace multi-space with single space \n",
    "    clean_strng = re.sub(r' +', r' ', clean_strng)\n",
    "\n",
    "    return clean_strng\n",
    "\n",
    "\n",
    "def tokenize_stem_no_stopwords(strng):\n",
    "    return [stemmer.stem(w) for w in word_tokenize(strng) if w not in stopword_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase and remove punctuation \n",
    "df['processed_comment'] = df['comment'].str.lower()\n",
    "df['processed_comment'] = df['processed_comment'].apply(process_reddit_comment)\n",
    "df['processed_comment'] = df['processed_comment'].apply(punct_remover)\n",
    "df['processed_comment'] = df['processed_comment'].apply(tokenize_stem_no_stopwords)"
   ]
  },
  {
   "source": [
    "Sentiments are represented in the columns.  \n",
    "If a reddit post is classified as having x sentiment, then we represent it with an 1 in x column"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = df.shape[0]\n",
    "for emotion in enkman_mapping.keys():\n",
    "    df[emotion] = np.zeros((N,1), dtype=int)\n",
    "\n",
    "for emotion in enkman_mapping.keys():\n",
    "    df[emotion] = df['mapped_emotions'].apply(lambda x: 1 if emotion in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                 comment label       id  \\\n",
       "0      My favourite food is anything I didn't have to...  [27]  eebbqej   \n",
       "1      Now if he does off himself, everyone will thin...  [27]  ed00q6i   \n",
       "2                         WHY THE FUCK IS BAYLESS ISOING   [2]  eezlygj   \n",
       "3                            To make her feel threatened  [14]  ed7ypvh   \n",
       "4                                 Dirty Southern Wankers   [3]  ed0bdzj   \n",
       "...                                                  ...   ...      ...   \n",
       "43405  Added you mate well I’ve just got the bow and ...  [18]  edsb738   \n",
       "43406  Always thought that was funny but is it a refe...   [6]  ee7fdou   \n",
       "43407  What are you talking about? Anything bad that ...   [3]  efgbhks   \n",
       "43408            More like a baptism, with sexy results!  [13]  ed1naf8   \n",
       "43409                                    Enjoy the ride!  [17]  eecwmbq   \n",
       "\n",
       "           emotions mapped_emotions  \\\n",
       "0         [neutral]       [neutral]   \n",
       "1         [neutral]       [neutral]   \n",
       "2           [anger]         [anger]   \n",
       "3            [fear]          [fear]   \n",
       "4       [annoyance]         [anger]   \n",
       "...             ...             ...   \n",
       "43405        [love]           [joy]   \n",
       "43406   [confusion]      [surprise]   \n",
       "43407   [annoyance]         [anger]   \n",
       "43408  [excitement]           [joy]   \n",
       "43409         [joy]           [joy]   \n",
       "\n",
       "                                       processed_comment  anger  disgust  \\\n",
       "0                   [favourit, food, anyth, didnt, cook]      0        0   \n",
       "1      [everyon, think, he, laugh, screw, peopl, inst...      0        0   \n",
       "2                                   [fuck, bayless, iso]      1        0   \n",
       "3                                 [make, feel, threaten]      0        0   \n",
       "4                              [dirti, southern, wanker]      1        0   \n",
       "...                                                  ...    ...      ...   \n",
       "43405  [ad, mate, well, ’, got, bow, love, hunt, aspe...      0        0   \n",
       "43406              [alway, thought, funni, refer, anyth]      0        0   \n",
       "43407  [talk, anyth, bad, happen, fault, -, good, thing]      1        0   \n",
       "43408                      [like, baptism, sexi, result]      0        0   \n",
       "43409                                      [enjoy, ride]      0        0   \n",
       "\n",
       "       fear  joy  sadness  surprise  neutral  \n",
       "0         0    0        0         0        1  \n",
       "1         0    0        0         0        1  \n",
       "2         0    0        0         0        0  \n",
       "3         1    0        0         0        0  \n",
       "4         0    0        0         0        0  \n",
       "...     ...  ...      ...       ...      ...  \n",
       "43405     0    1        0         0        0  \n",
       "43406     0    0        0         1        0  \n",
       "43407     0    0        0         0        0  \n",
       "43408     0    1        0         0        0  \n",
       "43409     0    1        0         0        0  \n",
       "\n",
       "[43410 rows x 13 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment</th>\n      <th>label</th>\n      <th>id</th>\n      <th>emotions</th>\n      <th>mapped_emotions</th>\n      <th>processed_comment</th>\n      <th>anger</th>\n      <th>disgust</th>\n      <th>fear</th>\n      <th>joy</th>\n      <th>sadness</th>\n      <th>surprise</th>\n      <th>neutral</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>My favourite food is anything I didn't have to...</td>\n      <td>[27]</td>\n      <td>eebbqej</td>\n      <td>[neutral]</td>\n      <td>[neutral]</td>\n      <td>[favourit, food, anyth, didnt, cook]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Now if he does off himself, everyone will thin...</td>\n      <td>[27]</td>\n      <td>ed00q6i</td>\n      <td>[neutral]</td>\n      <td>[neutral]</td>\n      <td>[everyon, think, he, laugh, screw, peopl, inst...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>WHY THE FUCK IS BAYLESS ISOING</td>\n      <td>[2]</td>\n      <td>eezlygj</td>\n      <td>[anger]</td>\n      <td>[anger]</td>\n      <td>[fuck, bayless, iso]</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>To make her feel threatened</td>\n      <td>[14]</td>\n      <td>ed7ypvh</td>\n      <td>[fear]</td>\n      <td>[fear]</td>\n      <td>[make, feel, threaten]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Dirty Southern Wankers</td>\n      <td>[3]</td>\n      <td>ed0bdzj</td>\n      <td>[annoyance]</td>\n      <td>[anger]</td>\n      <td>[dirti, southern, wanker]</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>43405</th>\n      <td>Added you mate well I’ve just got the bow and ...</td>\n      <td>[18]</td>\n      <td>edsb738</td>\n      <td>[love]</td>\n      <td>[joy]</td>\n      <td>[ad, mate, well, ’, got, bow, love, hunt, aspe...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>43406</th>\n      <td>Always thought that was funny but is it a refe...</td>\n      <td>[6]</td>\n      <td>ee7fdou</td>\n      <td>[confusion]</td>\n      <td>[surprise]</td>\n      <td>[alway, thought, funni, refer, anyth]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>43407</th>\n      <td>What are you talking about? Anything bad that ...</td>\n      <td>[3]</td>\n      <td>efgbhks</td>\n      <td>[annoyance]</td>\n      <td>[anger]</td>\n      <td>[talk, anyth, bad, happen, fault, -, good, thing]</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>43408</th>\n      <td>More like a baptism, with sexy results!</td>\n      <td>[13]</td>\n      <td>ed1naf8</td>\n      <td>[excitement]</td>\n      <td>[joy]</td>\n      <td>[like, baptism, sexi, result]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>43409</th>\n      <td>Enjoy the ride!</td>\n      <td>[17]</td>\n      <td>eecwmbq</td>\n      <td>[joy]</td>\n      <td>[joy]</td>\n      <td>[enjoy, ride]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>43410 rows × 13 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(df, random_state=156, test_size=0.25, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer()\n",
    "\n",
    "x_train = tfidf.fit_transform(X_train['processed_comment'].apply(lambda x: ' '.join(x)))\n",
    "x_test = tfidf.transform(X_test['processed_comment'].apply(lambda x: ' '.join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}